{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "intro-title",
      "metadata": {},
      "source": [
        "# ðŸ“˜ Agentic Architectures 2: Tool Use\n",
        "\n",
        "This notebook covers the second, and arguably one of the most transformative, agentic architectures: **Tool Use**. This pattern is the bridge that connects a Large Language Model's reasoning capabilities to the real, dynamic world.\n",
        "\n",
        "Without tools, an LLM is a closed system, limited by the knowledge frozen into its training data. It cannot know today's weather, the current price of a stock, or the status of an order in your company's database. By giving an agent the ability to use tools, we empower it to overcome this fundamental limitation, allowing it to query APIs, search databases, and access live information to provide answers that are not just reasoned, but also factual, timely, and relevant."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "intro-definition",
      "metadata": {},
      "source": [
        "### Definition\n",
        "The **Tool Use** architecture equips an LLM-powered agent with the ability to call external functions or APIs (the \"tools\"). The agent autonomously decides when a user's query cannot be answered by its internal knowledge alone and determines which tool is appropriate to call to find the necessary information.\n",
        "\n",
        "### High-level Workflow\n",
        "\n",
        "1.  **Receive Query:** The agent receives a request from the user.\n",
        "2.  **Decision:** The agent analyzes the query and its available tools. It decides if a tool is needed to answer the question accurately.\n",
        "3.  **Action:** If a tool is needed, the agent formats a call to that tool (e.g., a specific function with the right arguments).\n",
        "4.  **Observation:** The system executes the tool call, and the result (the \"observation\") is returned to the agent.\n",
        "5.  **Synthesis:** The agent integrates the tool's output into its reasoning process to generate a final, grounded answer for the user.\n",
        "\n",
        "### When to Use / Applications\n",
        "*   **Research Assistants:** Answering questions that require up-to-the-minute information by using a web search API.\n",
        "*   **Enterprise Assistants:** Querying internal company databases to answer questions like \"How many new users signed up last week?\"\n",
        "*   **Scientific & Mathematical Tasks:** Using a calculator or a computational engine like WolframAlpha for precise calculations that LLMs often struggle with.\n",
        "\n",
        "### Strengths & Weaknesses\n",
        "*   **Strengths:**\n",
        "    *   **Factual Grounding:** Drastically reduces hallucinations by fetching real, live data.\n",
        "    *   **Extensibility:** The agent's capabilities can be continuously expanded by simply adding new tools.\n",
        "*   **Weaknesses:**\n",
        "    *   **Integration Overhead:** Requires careful \"plumbing\" to define tools, handle API keys, and manage potential tool failures.\n",
        "    *   **Tool Trust:** The quality of the agent's answer is dependent on the reliability and accuracy of the tools it uses. The agent must trust that its tools provide correct information."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "phase0-title",
      "metadata": {},
      "source": [
        "## Phase 0: Foundation & Setup\n",
        "\n",
        "As before, we begin by setting up our environment. This includes installing the necessary libraries and configuring our API keys for Together AI, LangSmith, and the specific tool we will be using."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "setup-what",
      "metadata": {},
      "source": [
        "### Step 0.1: Installing Core Libraries\n",
        "\n",
        "**What we are going to do:**\n",
        "We'll install our standard set of libraries for orchestration (`langchain-together`, `langgraph`), environment management (`python-dotenv`), and printing (`rich`). Crucially, we will also install `tavily-python`, which provides a simple-to-use API for a powerful web search tool that we will give to our agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "install-libs",
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install -q -U langchain-together langchain langgraph rich python-dotenv tavily-python"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "imports-what",
      "metadata": {},
      "source": [
        "### Step 0.2: Importing Libraries and Setting Up Keys\n",
        "\n",
        "**What we are going to do:**\n",
        "We will import the necessary modules and use `python-dotenv` to load our API keys. For this notebook, we need keys for Together AI (for the LLM), LangSmith (for tracing), and Tavily (for the web search tool)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "import-and-keys",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from typing import List, Annotated, TypedDict, Optional\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# LangChain components\n",
        "from langchain_together import ChatTogether\n",
        "from langchain_tavily import TavilySearch\n",
        "from langchain_tavily.tavily_search import TavilySearchInput\n",
        "from langchain_core.messages import BaseMessage, ToolMessage\n",
        "from langchain_core.tools import StructuredTool\n",
        "from pydantic import BaseModel, Field, field_validator\n",
        "\n",
        "# LangGraph components\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.graph.message import AnyMessage, add_messages\n",
        "from langgraph.prebuilt import ToolNode\n",
        "\n",
        "# For pretty printing\n",
        "from rich.console import Console\n",
        "from rich.markdown import Markdown\n",
        "\n",
        "# --- API Key and Tracing Setup ---\n",
        "load_dotenv()\n",
        "\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = \"02_TIDIT_Workshop\"\n",
        "\n",
        "# Check that the keys are set\n",
        "for key in [\"TOGETHER_API_KEY\", \"LANGSMITH_API_KEY\", \"TAVILY_API_KEY\"]:\n",
        "    if not os.environ.get(key):\n",
        "        print(f\"{key} not found. Please create a .env file and set it.\")\n",
        "\n",
        "print(\"Environment variables loaded and tracing is set up.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "phase1-title",
      "metadata": {},
      "source": [
        "## Phase 1: Defining the Agent's Toolkit\n",
        "\n",
        "An agent is only as capable as the tools it has access to. In this phase, we will define and test the specific tool we'll give our agent: a live web search."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tool-what",
      "metadata": {},
      "source": [
        "### Step 1.1: Creating and Testing the Web Search Tool\n",
        "\n",
        "**What we are going to do:**\n",
        "We will instantiate the `TavilySearchResults` tool. The most critical part of defining a tool is its **description**. The LLM uses this natural language description to understand what the tool does and when it should be used. A clear, precise description is essential for the agent to make correct decisions. We will then test the tool directly to see what its raw output looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "create-tool",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the tool. We can set the max number of results to keep the context concise.\n",
        "tavily_tool = TavilySearch(max_results=2, tavily_api_key=os.environ[\"TAVILY_API_KEY\"])\n",
        "\n",
        "# It's crucial to give the tool a clear name and description for the agent\n",
        "tavily_tool.name = \"web_search\"\n",
        "tavily_tool.description = \"A tool that can be used to search the internet for up-to-date information on any topic, including news, events, and current affairs.\"\n",
        "\n",
        "\n",
        "# Coerce LLM-sent \"null\" (string) to None for optional Literal fields so TavilySearchInput validation succeeds.\n",
        "class SanitizedTavilySearchInput(TavilySearchInput):\n",
        "    @field_validator(\"time_range\", \"topic\", \"search_depth\", mode=\"before\")\n",
        "    @classmethod\n",
        "    def coerce_null_to_none(cls, v):\n",
        "        if v is None or v == \"null\":\n",
        "            return None\n",
        "        return v\n",
        "\n",
        "\n",
        "# Wrap so tool calls with time_range=\"null\" (or similar) are sanitized before Tavily validation.\n",
        "search_tool = StructuredTool.from_function(\n",
        "    func=lambda **kwargs: tavily_tool.invoke(kwargs),\n",
        "    name=tavily_tool.name,\n",
        "    description=tavily_tool.description,\n",
        "    args_schema=SanitizedTavilySearchInput,\n",
        ")\n",
        "tools = [search_tool]\n",
        "print(f\"Tool '{search_tool.name}' created with description: '{search_tool.description}'\")\n",
        "\n",
        "console = Console()\n",
        "\n",
        "# Let's test the tool directly to see its output format\n",
        "print(\"\\n--- Testing the tool directly ---\")\n",
        "test_query = \"What was the score of the last Wimbledon tournament?\"\n",
        "test_result = search_tool.invoke({\"query\": test_query})\n",
        "console.print(f\"[bold green]Query:[/bold green] {test_query}\")\n",
        "console.print(\"\\n[bold green]Result:[/bold green]\")\n",
        "console.print(test_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tool-discuss",
      "metadata": {},
      "source": [
        "**Discussion of the Output:**\n",
        "The test shows the raw output of our `web_search` tool. It returns a list of dictionaries, where each dictionary contains the URL and content snippet of a search result. This structured information is exactly what the agent will receive as its \"observation\" after it decides to use the tool. Now that we have a functional tool, we can build the agent that will learn how to use it."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "phase2-title",
      "metadata": {},
      "source": [
        "## Phase 2: Building the Tool-Using Agent with LangGraph\n",
        "\n",
        "Now we will construct the agentic workflow. This involves making the LLM aware of the tools and creating a graph that allows it to loop through a \"think-act-observe\" cycle, which is the essence of tool use."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "graph-state-what",
      "metadata": {},
      "source": [
        "### Step 2.1: Defining the Graph State\n",
        "\n",
        "**What we are going to do:**\n",
        "The state for a tool-using agent is typically a list of messages that represents the conversation history. This history includes the user's questions, the agent's thoughts and tool calls, and the results from those tools. We will use a `TypedDict` that can hold any type of LangChain message."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "graph-state-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[list[AnyMessage], add_messages]\n",
        "\n",
        "print(\"AgentState TypedDict defined to manage conversation history.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "llm-bind-what",
      "metadata": {},
      "source": [
        "### Step 2.2: Binding the Tools to the LLM\n",
        "\n",
        "**What we are going to do:**\n",
        "This is the critical step where we make the LLM \"aware\" of the tools. We use the `.bind_tools()` method, which passes the names and descriptions of our tools to the LLM's system prompt. This allows the model's internal logic to decide when to call a tool based on its description."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "llm-bind-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "llm = ChatTogether(\n",
        "    model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\",\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "# Bind the tools to the LLM, making it tool-aware\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "print(\"LLM has been bound with the provided tools.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nodes-what",
      "metadata": {},
      "source": [
        "### Step 2.3: Defining the Agent Nodes\n",
        "\n",
        "**What we are going to do:**\n",
        "Our graph will have two main nodes:\n",
        "1.  **`agent_node`:** This is the \"brain\". It calls the LLM with the current conversation history. The LLM's response will either be a final answer or a request to call a tool.\n",
        "2.  **`tool_node`:** This is the \"hands\". It takes the tool call request from the `agent_node`, executes the corresponding tool, and returns the output. We will use LangGraph's pre-built `ToolNode` for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "define-nodes",
      "metadata": {},
      "outputs": [],
      "source": [
        "def agent_node(state: AgentState):\n",
        "    \"\"\"The primary node that calls the LLM to decide the next action.\"\"\"\n",
        "    console.print(\"--- AGENT: Thinking... ---\")\n",
        "    response = llm_with_tools.invoke(state[\"messages\"])\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "# The ToolNode is a pre-built node from LangGraph that executes tools\n",
        "tool_node = ToolNode(tools)\n",
        "\n",
        "print(\"Agent node and Tool node have been defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "router-what",
      "metadata": {},
      "source": [
        "### Step 2.4: Defining the Conditional Router\n",
        "\n",
        "**What we are going to do:**\n",
        "After the `agent_node` runs, we need to decide where to go next. The router function inspects the last message from the agent. If that message contains a `tool_calls` attribute, it means the agent wants to use a tool, so we route to the `tool_node`. If not, it means the agent has a final answer, and we can end the workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "router-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def router_function(state: AgentState) -> str:\n",
        "    \"\"\"Inspects the agent's last message to decide the next step.\"\"\"\n",
        "    last_message = state[\"messages\"][-1]\n",
        "    if last_message.tool_calls:\n",
        "        # The agent has requested a tool call\n",
        "        console.print(\"--- ROUTER: Decision is to call a tool. ---\")\n",
        "        return \"call_tool\"\n",
        "    else:\n",
        "        # The agent has provided a final answer\n",
        "        console.print(\"--- ROUTER: Decision is to finish. ---\")\n",
        "        return \"__end__\"\n",
        "\n",
        "print(\"Router function defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "phase3-title",
      "metadata": {},
      "source": [
        "## Phase 3: Assembling and Running the Workflow\n",
        "\n",
        "Now we'll wire all the components together into a complete, executable graph and run it on a query that forces the agent to use its new web search capability."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "build-graph-what",
      "metadata": {},
      "source": [
        "### Step 3.1: Building and Visualizing the Graph\n",
        "\n",
        "**What we are going to do:**\n",
        "We will create the `StateGraph` and add our nodes and edges. The key part is the conditional edge that uses our `router_function` to create the agent's primary reasoning loop: `agent -> router -> tool -> agent`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "build-graph-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "graph_builder = StateGraph(AgentState)\n",
        "\n",
        "# Add the nodes\n",
        "graph_builder.add_node(\"agent\", agent_node)\n",
        "graph_builder.add_node(\"call_tool\", tool_node)\n",
        "\n",
        "# Set the entry point\n",
        "graph_builder.set_entry_point(\"agent\")\n",
        "\n",
        "# Add the conditional router\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    router_function,\n",
        ")\n",
        "\n",
        "# Add the edge from the tool node back to the agent to complete the loop\n",
        "graph_builder.add_edge(\"call_tool\", \"agent\")\n",
        "\n",
        "# Compile the graph\n",
        "tool_agent_app = graph_builder.compile()\n",
        "\n",
        "print(\"Tool-using agent graph compiled successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "build-graph-discuss",
      "metadata": {},
      "source": [
        "**Discussion of the Output:**\n",
        "The compiled graph is ready. The visualization clearly shows the agent's reasoning loop. The process starts at the `agent` node. The conditional edge (represented by the diamond) then routes the flow. If a tool is needed, it goes to `call_tool`, and the output is fed back to the `agent` for synthesis. If no tool is needed, the process goes to `__end__`. This structure perfectly implements the Tool Use pattern."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "execute-what",
      "metadata": {},
      "source": [
        "### Step 3.2: End-to-End Execution\n",
        "\n",
        "**What we are going to do:**\n",
        "Let's run the agent with a question that it cannot possibly know from its training data, forcing it to use the web search tool. We will stream the intermediate steps to watch its reasoning process unfold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "execute-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "user_query = \"What is the aim of the TIDIT Workshop: Agentic AI verstehen, gestalten und wirksam einsetzen?\"\n",
        "initial_input = {\"messages\": [(\"user\", user_query)]}\n",
        "\n",
        "console.print(f\"[bold cyan]ðŸš€ Kicking off Tool Use workflow for request:[/bold cyan] '{user_query}'\\n\")\n",
        "\n",
        "final_output = None\n",
        "for chunk in tool_agent_app.stream(initial_input, stream_mode=\"values\"):\n",
        "    final_output = chunk\n",
        "    chunk[\"messages\"][-1].pretty_print()\n",
        "    console.print(\"\\n---\\n\")\n",
        "\n",
        "console.print(\"\\n[bold green]âœ… Tool Use workflow complete![/bold green]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "phase4-title",
      "metadata": {},
      "source": [
        "## Phase 4: Evaluation\n",
        "\n",
        "Now that the agent has run, we can evaluate its performance. For a tool-using agent, we care about two things: did it use its tools correctly, and was the final answer, which was synthesized from the tool's output, high-quality?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eval-analysis-what",
      "metadata": {},
      "source": [
        "### Step 4.1: Analyzing the Execution Trace\n",
        "\n",
        "**What we are going to do:**\n",
        "By looking at the streamed output from the previous step, we can trace the agent's exact thought process. The output shows the different message types (`AIMessage` with `tool_calls`, `ToolMessage` with results) that flow through the graph state."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eval-analysis-discuss",
      "metadata": {},
      "source": [
        "**Discussion of the Output:**\n",
        "The execution trace clearly shows the Tool Use pattern in action:\n",
        "1.  The first message printed is from the `agent` node. It's an `AIMessage` containing a `tool_calls` attribute, indicating the LLM correctly decided to use the `web_search` tool.\n",
        "2.  The next message is a `ToolMessage`. This is the output from the `tool_node` after it executed the search and returned the raw results.\n",
        "3.  The final message is another `AIMessage`, but this time without `tool_calls`. This is the agent synthesizing the information from the `ToolMessage` into a coherent, final answer for the user.\n",
        "This trace confirms that the agent's logic and the graph's routing worked perfectly."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eval-judge-what",
      "metadata": {},
      "source": [
        "### Step 4.2: Evaluating with LLM-as-a-Judge\n",
        "\n",
        "**What we are going to do:**\n",
        "We will create a 'Judge' LLM to provide a structured, quantitative evaluation of the agent's performance. The evaluation criteria will be tailored specifically to assess the quality of tool use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eval-judge-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "class ToolUseEvaluation(BaseModel):\n",
        "    \"\"\"Schema for evaluating the agent's tool use and final answer.\"\"\"\n",
        "    tool_selection_score: int = Field(description=\"Score 1-10 on whether the agent chose the correct tool for the task.\")\n",
        "    tool_input_score: int = Field(description=\"Score 1-10 on how well-formed and relevant the input to the tool was.\")\n",
        "    synthesis_quality_score: int = Field(description=\"Score 1-10 on how well the agent integrated the tool's output into its final answer.\")\n",
        "    justification: str = Field(description=\"A brief justification for the scores.\")\n",
        "\n",
        "judge_llm = llm.with_structured_output(ToolUseEvaluation)\n",
        "\n",
        "# Reuse the output captured during the streaming run â€” no extra API call needed\n",
        "conversation_trace = \"\\n\".join([f\"{m.type}: {m.content or ''} {getattr(m, 'tool_calls', '')}\" for m in final_output['messages']])\n",
        "\n",
        "def evaluate_tool_use(trace: str):\n",
        "    prompt = f\"\"\"You are an expert judge of AI agents. Evaluate the following conversation trace based on the agent's tool use on a scale of 1-10. Provide a brief justification.\n",
        "    \n",
        "    Conversation Trace:\n",
        "    ```\n",
        "    {trace}\n",
        "    ```\n",
        "    \"\"\"\n",
        "    return judge_llm.invoke(prompt)\n",
        "\n",
        "console.print(\"--- Evaluating Tool Use Performance ---\")\n",
        "evaluation = evaluate_tool_use(conversation_trace)\n",
        "console.print(evaluation.model_dump())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eval-judge-discuss",
      "metadata": {},
      "source": [
        "**Discussion of the Output:**\n",
        "The LLM-as-a-Judge provides a structured and reasoned assessment of our agent's performance. The high scores across all three categoriesâ€”`tool_selection_score`, `tool_input_score`, and `synthesis_quality_score`â€”confirm that our agent is not just using tools, but using them *effectively*. It correctly identified the need for a web search, formulated a relevant query, and successfully synthesized the retrieved facts into a helpful and accurate final answer. This automated evaluation gives us confidence in the robustness of our implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "conclusion",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook, we have built a complete, functioning agent based on the **Tool Use** architecture. We successfully equipped a LLM with a web search tool and used LangGraph to create a robust reasoning loop that allows the agent to decide when and how to use it.\n",
        "\n",
        "The end-to-end execution and subsequent evaluation demonstrate the immense value of this pattern. By connecting our agent to live, external information, we have fundamentally overcome the limitation of static training data. The agent is no longer just a reasoner; it is a researcher, capable of providing answers that are grounded, factual, and current. This architecture is a foundational building block for creating virtually any practical, real-world AI assistant."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
